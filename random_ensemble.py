# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AXb5EqO_x1pn9cpIDaNskL4J_PGaBKj3
"""

import os
import sys
from VGG import create_VGG
import torch
import torchvision
from torch.utils import data
from torch import optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from torchvision import transforms
import random
from torch import nn
import torch.nn.functional as F
import math

'''
Pipeline
1. Read dataset
2. Use specific sample rate (40%, 50%, 60%) to sample a sub dataset for training
3. Use the sampled training data to train five models respectively to get a model pool with size 5
4. Test the model pool's performance and diversity:
    (1) testing performance is straightforward，just calculate the testing acc of five models and then take the avg
    (2) testing diversity:
        - select 20 test samples from class of the testing set. CIFAR-10 has 10 classes, so there are totally 200 samples
        - go through these 200 samples one by one and each of them will be fed into model_1, model_2 ... model_5
        - backpropagate to get the gradident of the current input after each sample is fed into the model, we'll get five such gradients G1, G2 ..., G5
        - calculate the cosine similarity of each pair of gradients (G1G2,G1G3,G1G4... then we'll get 10 cosine similarity), calculate the sum to get a similarity sum
        - each of the 200 samples will have such a similarity sum
          so there are 200 such similarity sum. Sum them up to get a quantity which could represent the model pool's diversity
5. Go back to step2，adjust sample rate and repeat step 3 and 4. Finally get the performance and diversity of the model pool with 40%, 50%, 60% sample rate
   Analyze the result to see which sample rate should be selected
'''

transform = transforms.Compose([transforms.ToTensor()])
device = torch.device("cuda")


# Sample 40%, 50%, 60% training data
def sample_train_data(sample_rate, train_dataset):
    n_train = len(train_dataset)
    split = int(n_train * sample_rate)
    indices = list(range(n_train))
    random.shuffle(indices)
    randomsampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])
    train_loader = data.DataLoader(train_dataset, batch_size=64, pin_memory=True, sampler=randomsampler)

    return train_loader


# Train each model in the pool with sampled data (40%, 50%, 60%)
def train_model_pool(num_models, sample_rate, train_raw, test_loader):
    # Initialize the model pool
    model_pool_pre = []
    for i in range(num_models):
        model = create_VGG('VGG19', 10)
        model.to(device)
        model_pool_pre.append(model)

    model_pool = []
    # Each model is trained with a different random sample
    for i, model in enumerate(model_pool_pre):
        print("train model {} / {}".format(i + 1, num_models))
        sampled_train_loader = sample_train_data(sample_rate, train_raw)
        model = train(model, sampled_train_loader, test_loader)
        model_pool.append(model)

    return model_pool


# Train a single model using sampled data
def train(model, train_loader, test_loader):
    n_epoches = 50
    optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0006, momentum=0.9)
    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=3, verbose=True, min_lr=1.0e-04)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(n_epoches):
        avg_loss = train_epoch(model, train_loader, optimizer, criterion)
        acc = test(model, test_loader)
        print("Epoch:", epoch + 1)
        print('Train Loss: ', round(avg_loss, 5))
        print('acc: {}'.format(acc))
        scheduler.step(avg_loss)

    return model


def train_epoch(model, train_loader, optimizer, criterion):
    model.train()
    running_loss = 0.0
    total_correct = 0

    scaler = torch.cuda.amp.GradScaler()
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        with torch.cuda.amp.autocast():
            output = model(x)
            loss = criterion(output, y)
            running_loss += loss.item()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    avg_loss = running_loss / len(train_loader)
    return avg_loss


def save_model(model_pool, model_dir):
    for i, model in enumerate(model_pool):
        torch.save(model.state_dict(), os.path.join(model_dir, f'model{i + 1}_parameter.pkl'))


def resume_model(num_model, model_dir):
    model_pool = []
    for i in range(num_model):
        model = create_VGG('VGG19', 10).to(device)
        model.load_state_dict(torch.load(os.path.join(model_dir, f'model{i + 1}_parameter.pkl'))
        model_pool.append(model)
    return model_pool


# Test one model to get its testing accuracy
def test(model, test_loader):
    model.eval()
    running_loss = 0.0
    total_correct = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)

            # Predict
            output = model(x)
            y_hat = output.argmax(dim=1)

            # Calculate loss and the number of correct predictions
            loss = criterion(output, y)
            running_loss += loss.item()
            correct = (y_hat == y).float().sum()
            total_correct += correct.cpu().detach().numpy()
    return total_correct / len(test_loader.dataset)


# Get 20 samples from each class to calculate model pool's diversity
def generate_test_samples(test_raw, num_sample=20):
    sample = []
    for i in range(10):
        sample.append([])
        idx = np.flatnonzero(np.array(test_raw.targets) == i)
        idx = np.random.choice(idx, num_sample, replace=False)
        for j in idx:
            sample[i].append((transform(test_raw.data[j]), torch.tensor([test_raw.targets[j]])))
    return sample


# Calculate the diversity of the current model pool
def cal_correlation(model_pool, test_samples):
    for model in model_pool:
        model.eval()
    pearson_correlation = 0
    cosine_correlation = 0
    criterion = nn.CrossEntropyLoss()
    count = 0
    for i in range(10):
        for j, (x, y) in enumerate(test_samples[i]):
            grad_pool = []
            pearson_correlation_temp = 0
            cosine_correlation_temp = 0
            for model in model_pool:
                x, y = x.to(device), y.to(device)
                x.requires_grad = True
                output = model(x.unsqueeze(0))
                loss = criterion(output, y)
                model.zero_grad()
                loss.backward()
                data_grad = x.grad.data
                grad_pool.append(data_grad.cpu().detach().numpy())
            grad_pool = [grad.flatten() for grad in grad_pool]
            for k in grad_pool:
                for l in grad_pool:
                    if ((k == l).all()):
                        continue
                    pearson_correlation_temp += np.corrcoef(k, l)[0][1]
                    cosine_correlation_temp += (k * l).sum() / (np.linalg.norm(k) * np.linalg.norm(l))
                    count += 1
            pearson_correlation += (pearson_correlation_temp / 2)
            cosine_correlation += (cosine_correlation_temp / 2)
    avg_pearson = pearson_correlation / count
    avg_cosine = cosine_correlation / count
    return pearson_correlation, cosine_correlation, avg_pearson, avg_cosine


def main(sample_rate, output_dir):
    train_raw = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    test_raw = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

    # Create data loader
    test_loader = torch.utils.data.DataLoader(test_raw, batch_size=64, shuffle=True, pin_memory=True)

    num_model = 20
    # sample_rate = 0.5  # could change this value to 0.3 0.4 0.5 0.6
    model_pool = train_model_pool(num_model, sample_rate, train_raw, test_loader)

    save_model(model_pool, output_dir)

    ##############################  SAVE MODEL AND RESUME MODEL LINE  #################################

    model_pool = resume_model(num_model, output_dir)

    # Test each model in the pool to get its testing accuracy
    ls_test_acc = []
    for model in model_pool:
        ls_test_acc.append(test(model, test_loader))
    avg_test_acc = sum(ls_test_acc) / len(ls_test_acc)

    # Generate test samples for testing the diversity of model
    num_sample = 20
    test_samples = generate_test_samples(test_raw, num_sample)

    # Calculate the model pool's diversity using the above generated test samples
    pearson_correlation, cosine_correlation, avg_pearson, \
    avg_cosine = cal_correlation(model_pool, test_samples)

    print("avg_test_acc: {}".format(avg_test_acc))
    print("pearson correlation: {}, cosine correlation: {}, "
          "average pearson: {},average cosine: {} ".format(pearson_correlation,
                                                           cosine_correlation, avg_pearson, avg_cosine))


if __name__ == '__main__':
    SAMPLE_RATE = sys.argv[1]
    OUTPUT_DIR = sys.argv[2]
    main(SAMPLE_RATE, OUTPUT_DIR)
